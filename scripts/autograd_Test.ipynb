{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "/home/robot/miniconda3/envs/bonsai/bin/python\n"
    }
   ],
   "source": [
    "%%bash\n",
    "which python\n",
    "#conda install pytorch torchvision cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3.7.5 (default, Oct 25 2019, 15:51:11) \n[GCC 7.3.0]\n"
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import PyTorch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.autograd import Function # import Function to create custom activations\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import datasets, transforms # import transformations to use for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\ntensor([[3., 3.],\n        [3., 3.]], grad_fn=<AddBackward0>)\n<AddBackward0 object at 0x7f9f1398b790>\nz, out  tensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\ntensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])\n"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(\"z, out \",z, out)\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "False\nTrue\n<SumBackward0 object at 0x7f9f118ac050>\ntensor([[ 2.4886, -1.4162],\n        [30.6073, -0.5104]])\n"
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "# print(b.requires_grad)\n",
    "print(b.grad_fn)\n",
    "b.backward()\n",
    "print (a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([ 911.4273, -309.8727,  663.8610], grad_fn=<MulBackward0>)\n"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
    }
   ],
   "source": [
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True\nTrue\nFalse\n"
    }
   ],
   "source": [
    "\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 36827200.0\n1 37136068.0\n2 37054992.0\n3 31112332.0\n4 20757048.0\n5 11267230.0\n6 5671826.5\n7 3041493.25\n8 1889423.75\n9 1347101.625\n10 1050983.0\n11 861334.9375\n12 724215.75\n13 617668.25\n14 531586.125\n15 460542.0625\n16 401076.75\n17 350844.8125\n18 308197.46875\n19 271687.0625\n20 240277.96875\n21 213173.984375\n22 189676.0\n23 169241.53125\n24 151383.09375\n25 135732.015625\n26 121961.71875\n27 109812.71875\n28 99065.0\n29 89532.171875\n30 81055.453125\n31 73503.8203125\n32 66759.6484375\n33 60725.0703125\n34 55315.26953125\n35 50456.703125\n36 46088.8203125\n37 42157.453125\n38 38609.21875\n39 35405.9375\n40 32505.271484375\n41 29876.99609375\n42 27494.169921875\n43 25327.787109375\n44 23355.5703125\n45 21556.97265625\n46 19914.955078125\n47 18414.546875\n48 17042.482421875\n49 15785.951171875\n50 14633.884765625\n51 13577.021484375\n52 12607.12890625\n53 11714.798828125\n54 10893.4853515625\n55 10137.3427734375\n56 9439.8125\n57 8796.6923828125\n58 8203.130859375\n59 7654.50244140625\n60 7147.23486328125\n61 6677.9580078125\n62 6243.083984375\n63 5839.96044921875\n64 5466.19970703125\n65 5119.0087890625\n66 4796.39794921875\n67 4496.53125\n68 4217.66796875\n69 3958.04833984375\n70 3716.2177734375\n71 3490.718994140625\n72 3280.376708984375\n73 3084.055908203125\n74 2900.730712890625\n75 2729.42138671875\n76 2569.323486328125\n77 2419.47119140625\n78 2279.28759765625\n79 2148.109375\n80 2025.123291015625\n81 1909.884033203125\n82 1801.84912109375\n83 1700.51220703125\n84 1605.3585205078125\n85 1516.0224609375\n86 1432.118408203125\n87 1353.27099609375\n88 1279.148681640625\n89 1209.424072265625\n90 1143.819091796875\n91 1082.109619140625\n92 1023.9669189453125\n93 969.2561645507812\n94 917.6892700195312\n95 869.0899658203125\n96 823.2440185546875\n97 780.0032958984375\n98 739.231201171875\n99 700.7340698242188\n100 664.399658203125\n101 630.0845947265625\n102 597.6683349609375\n103 567.0404663085938\n104 538.1078491210938\n105 510.74078369140625\n106 484.8526611328125\n107 460.3699035644531\n108 437.2017517089844\n109 415.2837219238281\n110 394.53302001953125\n111 374.88519287109375\n112 356.2625732421875\n113 338.6331481933594\n114 321.9283142089844\n115 306.0928955078125\n116 291.07733154296875\n117 276.8545837402344\n118 263.3525085449219\n119 250.54592895507812\n120 238.40518188476562\n121 226.8840789794922\n122 215.94064331054688\n123 205.55963134765625\n124 195.71128845214844\n125 186.34939575195312\n126 177.45834350585938\n127 169.01263427734375\n128 160.98573303222656\n129 153.3564910888672\n130 146.10806274414062\n131 139.2237091064453\n132 132.67352294921875\n133 126.44454193115234\n134 120.52714538574219\n135 114.89287567138672\n136 109.53597259521484\n137 104.4403076171875\n138 99.58924102783203\n139 94.97300720214844\n140 90.57888793945312\n141 86.39877319335938\n142 82.4179916381836\n143 78.62646484375\n144 75.0174560546875\n145 71.58151245117188\n146 68.30919647216797\n147 65.1902847290039\n148 62.219417572021484\n149 59.38859939575195\n150 56.69113540649414\n151 54.12139892578125\n152 51.67152404785156\n153 49.335105895996094\n154 47.10832595825195\n155 44.987709045410156\n156 42.96227264404297\n157 41.034297943115234\n158 39.19568634033203\n159 37.44025802612305\n160 35.76578140258789\n161 34.16813659667969\n162 32.64542007446289\n163 31.19235610961914\n164 29.805227279663086\n165 28.48283576965332\n166 27.21918487548828\n167 26.013296127319336\n168 24.863201141357422\n169 23.766319274902344\n170 22.717666625976562\n171 21.7163143157959\n172 20.762638092041016\n173 19.85009765625\n174 18.97877311706543\n175 18.146862030029297\n176 17.351810455322266\n177 16.59283447265625\n178 15.867114067077637\n179 15.175926208496094\n180 14.514209747314453\n181 13.882619857788086\n182 13.280672073364258\n183 12.705427169799805\n184 12.15450382232666\n185 11.628915786743164\n186 11.127032279968262\n187 10.646024703979492\n188 10.187201499938965\n189 9.748722076416016\n190 9.329280853271484\n191 8.927936553955078\n192 8.544355392456055\n193 8.17808723449707\n194 7.827714920043945\n195 7.492535591125488\n196 7.171972274780273\n197 6.865750789642334\n198 6.572195053100586\n199 6.291853427886963\n200 6.023937225341797\n201 5.767229080200195\n202 5.521801471710205\n203 5.2870635986328125\n204 5.062460899353027\n205 4.847888469696045\n206 4.641923427581787\n207 4.445464134216309\n208 4.25738000869751\n209 4.077404975891113\n210 3.9047622680664062\n211 3.739973545074463\n212 3.5823416709899902\n213 3.43110990524292\n214 3.2864131927490234\n215 3.148066282272339\n216 3.0157618522644043\n217 2.8888607025146484\n218 2.767545223236084\n219 2.6514177322387695\n220 2.5401828289031982\n221 2.4337809085845947\n222 2.331794261932373\n223 2.2340495586395264\n224 2.140760898590088\n225 2.0512566566467285\n226 1.9655139446258545\n227 1.8834290504455566\n228 1.8048971891403198\n229 1.7296091318130493\n230 1.6574995517730713\n231 1.5886017084121704\n232 1.5226093530654907\n233 1.4591730833053589\n234 1.3987220525741577\n235 1.3406068086624146\n236 1.2849584817886353\n237 1.2316348552703857\n238 1.1804486513137817\n239 1.1315832138061523\n240 1.0847089290618896\n241 1.0398986339569092\n242 0.996880292892456\n243 0.9556998014450073\n244 0.9161899089813232\n245 0.8784030675888062\n246 0.8422000408172607\n247 0.8075114488601685\n248 0.7741246819496155\n249 0.7422747015953064\n250 0.7117489576339722\n251 0.6823128461837769\n252 0.654356837272644\n253 0.627433717250824\n254 0.6017377376556396\n255 0.5770192742347717\n256 0.5533291697502136\n257 0.5306471586227417\n258 0.5088777542114258\n259 0.48804107308387756\n260 0.4680154621601105\n261 0.44888997077941895\n262 0.43050098419189453\n263 0.412929505109787\n264 0.39608412981033325\n265 0.37988993525505066\n266 0.3643498420715332\n267 0.34948474168777466\n268 0.3352886438369751\n269 0.3216363489627838\n270 0.3084968626499176\n271 0.2958647608757019\n272 0.2838670015335083\n273 0.2723139524459839\n274 0.2612222135066986\n275 0.2506018579006195\n276 0.2404276430606842\n277 0.23066623508930206\n278 0.221293643116951\n279 0.21232099831104279\n280 0.2036597579717636\n281 0.1954479068517685\n282 0.18753722310066223\n283 0.1799352765083313\n284 0.17264705896377563\n285 0.1656835824251175\n286 0.1589597910642624\n287 0.15250995755195618\n288 0.14634689688682556\n289 0.1404591202735901\n290 0.13479846715927124\n291 0.12933866679668427\n292 0.12412066012620926\n293 0.11910160630941391\n294 0.11429622024297714\n295 0.10970684885978699\n296 0.10525946319103241\n297 0.10105684399604797\n298 0.09698870778083801\n299 0.0931062251329422\n300 0.08934547752141953\n301 0.08576840162277222\n302 0.08229228854179382\n303 0.07899343967437744\n304 0.07584956288337708\n305 0.07281079143285751\n306 0.06988640874624252\n307 0.06707743555307388\n308 0.0643799677491188\n309 0.061807531863451004\n310 0.05932972580194473\n311 0.056958019733428955\n312 0.05467452108860016\n313 0.05251532047986984\n314 0.05038793757557869\n315 0.04837128892540932\n316 0.04644119367003441\n317 0.0445808470249176\n318 0.04282429441809654\n319 0.04111151024699211\n320 0.03947358578443527\n321 0.037888042628765106\n322 0.03638482093811035\n323 0.03493025153875351\n324 0.033556148409843445\n325 0.03221583366394043\n326 0.030932467430830002\n327 0.029704950749874115\n328 0.028540657833218575\n329 0.027400650084018707\n330 0.026303455233573914\n331 0.02527046389877796\n332 0.024288959801197052\n333 0.02331669256091118\n334 0.022398224100470543\n335 0.021521568298339844\n336 0.020677905529737473\n337 0.019864367321133614\n338 0.019089601933956146\n339 0.01833244413137436\n340 0.01761794649064541\n341 0.016928497701883316\n342 0.016270095482468605\n343 0.01563742570579052\n344 0.015030664391815662\n345 0.014442932792007923\n346 0.01388285867869854\n347 0.013343170285224915\n348 0.01282423548400402\n349 0.012330908328294754\n350 0.01185191236436367\n351 0.01139743346720934\n352 0.010966945439577103\n353 0.010539485141634941\n354 0.010136362165212631\n355 0.009745918214321136\n356 0.009375021792948246\n357 0.009011768735945225\n358 0.008666597306728363\n359 0.008342267014086246\n360 0.008029795251786709\n361 0.007726978976279497\n362 0.007425932213664055\n363 0.007149493787437677\n364 0.0068807462230324745\n365 0.006621792912483215\n366 0.006373893469572067\n367 0.006136509124189615\n368 0.005909047555178404\n369 0.0056884814985096455\n370 0.005482051521539688\n371 0.005282064434140921\n372 0.005087289493530989\n373 0.004901673179119825\n374 0.004720536060631275\n375 0.004551378078758717\n376 0.004384585190564394\n377 0.004223268944770098\n378 0.004076060838997364\n379 0.003931168932467699\n380 0.0037864232435822487\n381 0.003652755869552493\n382 0.0035236957482993603\n383 0.0034018326550722122\n384 0.0032844620291143656\n385 0.003167269052937627\n386 0.003062295028939843\n387 0.0029537694063037634\n388 0.0028551416471600533\n389 0.0027554775588214397\n390 0.0026635299436748028\n391 0.002574255922809243\n392 0.0024848917964845896\n393 0.0023985218722373247\n394 0.002320205792784691\n395 0.0022446149960160255\n396 0.0021699490025639534\n397 0.002100047655403614\n398 0.002031646203249693\n399 0.001966289710253477\n400 0.001902434160001576\n401 0.0018394344951957464\n402 0.0017812367295846343\n403 0.0017245723865926266\n404 0.0016708706971257925\n405 0.0016202135011553764\n406 0.001568418461829424\n407 0.0015201173955574632\n408 0.001472966163419187\n409 0.001427647890523076\n410 0.0013856729492545128\n411 0.0013440678594633937\n412 0.0013024660293012857\n413 0.0012649067211896181\n414 0.0012272765161469579\n415 0.0011894793715327978\n416 0.0011546092573553324\n417 0.0011212839744985104\n418 0.0010875455336645246\n419 0.0010575841879472136\n420 0.001027944264933467\n421 0.0009993131970986724\n422 0.0009705541888251901\n423 0.0009431276703253388\n424 0.0009171386482194066\n425 0.0008916731458157301\n426 0.0008668594527989626\n427 0.0008434525807388127\n428 0.0008197481511160731\n429 0.0007982862298376858\n430 0.0007772486424073577\n431 0.0007582127000205219\n432 0.0007368970545940101\n433 0.0007169786258600652\n434 0.0006969531532377005\n435 0.0006804657750762999\n436 0.0006637783953920007\n437 0.0006448575877584517\n438 0.0006284299306571484\n439 0.000612982374150306\n440 0.0005972979706712067\n441 0.0005820958176627755\n442 0.000568062998354435\n443 0.0005552299553528428\n444 0.0005417788634076715\n445 0.0005286409868858755\n446 0.0005154341924935579\n447 0.0005054851062595844\n448 0.0004930004361085594\n449 0.00048046285519376397\n450 0.0004690934729296714\n451 0.0004574290942400694\n452 0.0004470214480534196\n453 0.0004363550106063485\n454 0.000427529274020344\n455 0.0004173420602455735\n456 0.0004080606158822775\n457 0.00039874916546978056\n458 0.00038923934334889054\n459 0.0003807282482739538\n460 0.00037235365016385913\n461 0.00036458292743191123\n462 0.0003569801338016987\n463 0.0003491205570753664\n464 0.0003419517306610942\n465 0.0003346357843838632\n466 0.00032845581881701946\n467 0.00032105372520163655\n468 0.0003144294023513794\n469 0.00030792615143582225\n470 0.0003019035211764276\n471 0.00029492087196558714\n472 0.0002892902120947838\n473 0.0002824208640959114\n474 0.00027712894370779395\n475 0.00027166586369276047\n476 0.00026663270546123385\n477 0.0002611751260701567\n478 0.0002561676665209234\n479 0.0002514190855436027\n480 0.00024583944468759\n481 0.00024156860308721662\n482 0.00023740161850582808\n483 0.00023268762743100524\n484 0.00022889039246365428\n485 0.0002240654721390456\n486 0.00021954455587547272\n487 0.00021474610548466444\n488 0.00021170418767724186\n489 0.00020756038429681212\n490 0.00020389728888403624\n491 0.00019989827706012875\n492 0.00019591920136008412\n493 0.00019272571080364287\n494 0.0001895437453640625\n495 0.00018641028145793825\n496 0.00018320592062082142\n497 0.00018057445413433015\n498 0.0001764383923728019\n499 0.00017394570750184357\n"
    }
   ],
   "source": [
    "# Code in file tensor/two_layer_net_tensor.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.mm(w1)\n",
    "  h_relu = h.clamp(min=0)\n",
    "  y_pred = h_relu.mm(w2)\n",
    "\n",
    "  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "  # of shape (); we can get its value as a Python number with loss.item().\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "  grad_h = grad_h_relu.clone()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "  # Update weights using gradient descent\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 37477496.0\n1 35445524.0\n2 33695760.0\n3 27748754.0\n4 18673774.0\n5 10567113.0\n6 5553355.0\n7 3036548.75\n8 1847447.375\n9 1261153.75\n10 939651.25\n11 740273.8125\n12 602763.1875\n13 500131.71875\n14 420079.5625\n15 355967.25\n16 303665.78125\n17 260477.984375\n18 224481.859375\n19 194279.65625\n20 168823.078125\n21 147232.765625\n22 128828.375\n23 113059.671875\n24 99502.3125\n25 87799.1015625\n26 77680.6953125\n27 68892.8046875\n28 61235.890625\n29 54541.23046875\n30 48673.7578125\n31 43517.79296875\n32 38977.8125\n33 34968.5625\n34 31422.28515625\n35 28278.99609375\n36 25488.322265625\n37 23005.283203125\n38 20792.96875\n39 18820.1796875\n40 17056.083984375\n41 15475.1572265625\n42 14056.115234375\n43 12781.572265625\n44 11634.689453125\n45 10600.8779296875\n46 9667.771484375\n47 8824.771484375\n48 8061.95263671875\n49 7371.1123046875\n50 6744.4912109375\n51 6175.8173828125\n52 5659.02294921875\n53 5189.0732421875\n54 4761.1083984375\n55 4371.45166015625\n56 4016.202880859375\n57 3691.826904296875\n58 3395.05908203125\n59 3123.85693359375\n60 2875.820556640625\n61 2648.81298828125\n62 2440.99755859375\n63 2250.408447265625\n64 2075.620849609375\n65 1915.268310546875\n66 1768.01171875\n67 1632.7442626953125\n68 1508.48291015625\n69 1394.12939453125\n70 1288.914306640625\n71 1192.0421142578125\n72 1102.8646240234375\n73 1020.6580810546875\n74 944.9234619140625\n75 875.0664672851562\n76 810.6168212890625\n77 751.1050415039062\n78 696.1786499023438\n79 645.44140625\n80 598.5701904296875\n81 555.2528686523438\n82 515.1978759765625\n83 478.1466064453125\n84 443.87847900390625\n85 412.1630859375\n86 382.8000183105469\n87 355.6092529296875\n88 330.4317321777344\n89 307.09619140625\n90 285.4700012207031\n91 265.4173583984375\n92 246.82431030273438\n93 229.57778930664062\n94 213.57830810546875\n95 198.73939514160156\n96 184.96148681640625\n97 172.16720581054688\n98 160.28619384765625\n99 149.25204467773438\n100 139.00067138671875\n101 129.47682189941406\n102 120.62565612792969\n103 112.39676666259766\n104 104.74751281738281\n105 97.63294982910156\n106 91.01667785644531\n107 84.8606185913086\n108 79.13551330566406\n109 73.80418395996094\n110 68.84358215332031\n111 64.22361755371094\n112 59.92450714111328\n113 55.919654846191406\n114 52.19190216064453\n115 48.7171516418457\n116 45.47980880737305\n117 42.464027404785156\n118 39.65347671508789\n119 37.0330924987793\n120 34.59100341796875\n121 32.314083099365234\n122 30.18973159790039\n123 28.208660125732422\n124 26.36134910583496\n125 24.637561798095703\n126 23.02912139892578\n127 21.528728485107422\n128 20.127965927124023\n129 18.821044921875\n130 17.60109519958496\n131 16.461956024169922\n132 15.397586822509766\n133 14.40355110168457\n134 13.47626781463623\n135 12.609519004821777\n136 11.799829483032227\n137 11.043462753295898\n138 10.336092948913574\n139 9.675694465637207\n140 9.058402061462402\n141 8.480853080749512\n142 7.941124439239502\n143 7.436582565307617\n144 6.964485168457031\n145 6.5233259201049805\n146 6.110515117645264\n147 5.724847316741943\n148 5.363691329956055\n149 5.025867938995361\n150 4.7096357345581055\n151 4.4140706062316895\n152 4.137105941772461\n153 3.8777151107788086\n154 3.635265588760376\n155 3.4080779552459717\n156 3.1956119537353516\n157 2.9964585304260254\n158 2.8101232051849365\n159 2.6355323791503906\n160 2.4720215797424316\n161 2.3188889026641846\n162 2.1753134727478027\n163 2.0408005714416504\n164 1.9149659872055054\n165 1.79681396484375\n166 1.6862660646438599\n167 1.5825927257537842\n168 1.4855120182037354\n169 1.3943556547164917\n170 1.3089594841003418\n171 1.2288528680801392\n172 1.1538100242614746\n173 1.0833637714385986\n174 1.0173766613006592\n175 0.9553159475326538\n176 0.8973689675331116\n177 0.8429605960845947\n178 0.7917721271514893\n179 0.7438117265701294\n180 0.6988241672515869\n181 0.6565903425216675\n182 0.6169213056564331\n183 0.5797857046127319\n184 0.5448747873306274\n185 0.5121443271636963\n186 0.4813283085823059\n187 0.4523860216140747\n188 0.42524653673171997\n189 0.3998628556728363\n190 0.3759707510471344\n191 0.3534731864929199\n192 0.3323783278465271\n193 0.3125479221343994\n194 0.2939516007900238\n195 0.27647969126701355\n196 0.25999951362609863\n197 0.24461659789085388\n198 0.23011520504951477\n199 0.21648241579532623\n200 0.20365503430366516\n201 0.1916220337152481\n202 0.1802787482738495\n203 0.1696891337633133\n204 0.15966041386127472\n205 0.15028735995292664\n206 0.14144694805145264\n207 0.13316836953163147\n208 0.1253340244293213\n209 0.11798900365829468\n210 0.1111072227358818\n211 0.10457742214202881\n212 0.09846484661102295\n213 0.09272152185440063\n214 0.0873233899474144\n215 0.08221107721328735\n216 0.07741741836071014\n217 0.07290254533290863\n218 0.06868971884250641\n219 0.064703069627285\n220 0.060968801379203796\n221 0.05741826072335243\n222 0.05410109832882881\n223 0.05098144710063934\n224 0.04803477227687836\n225 0.045263662934303284\n226 0.04266771674156189\n227 0.04019859433174133\n228 0.03788873925805092\n229 0.0356982983648777\n230 0.03365751728415489\n231 0.031723473221063614\n232 0.029913416132330894\n233 0.028189241886138916\n234 0.026589438319206238\n235 0.025069627910852432\n236 0.0236373171210289\n237 0.022290829569101334\n238 0.021028678864240646\n239 0.019829977303743362\n240 0.018712924793362617\n241 0.017640862613916397\n242 0.016651879996061325\n243 0.015709275379776955\n244 0.014825332909822464\n245 0.01399085484445095\n246 0.013209071010351181\n247 0.012469394132494926\n248 0.011777798645198345\n249 0.011125648394227028\n250 0.01050327904522419\n251 0.00991221982985735\n252 0.009368770755827427\n253 0.008847926743328571\n254 0.008367984555661678\n255 0.007908189669251442\n256 0.007469392381608486\n257 0.007067089434713125\n258 0.006681780330836773\n259 0.006324860267341137\n260 0.005978081375360489\n261 0.005652295891195536\n262 0.005347160156816244\n263 0.005062263458967209\n264 0.004795227665454149\n265 0.0045369588769972324\n266 0.004296979866921902\n267 0.0040693748742341995\n268 0.0038566063158214092\n269 0.0036578394938260317\n270 0.003468838520348072\n271 0.003287093248218298\n272 0.003118084277957678\n273 0.002955942414700985\n274 0.002804005052894354\n275 0.0026608631014823914\n276 0.002529197372496128\n277 0.0024009225890040398\n278 0.0022783109452575445\n279 0.002165846060961485\n280 0.0020577479153871536\n281 0.001956174150109291\n282 0.00186122115701437\n283 0.0017692710971459746\n284 0.0016862726770341396\n285 0.0016046788077801466\n286 0.001528821885585785\n287 0.0014568560291081667\n288 0.001389624085277319\n289 0.0013260951964184642\n290 0.0012629857519641519\n291 0.001206966582685709\n292 0.001151772914454341\n293 0.001100323162972927\n294 0.001049127196893096\n295 0.0010038049658760428\n296 0.0009589141700416803\n297 0.0009178731124848127\n298 0.0008775537135079503\n299 0.00083988590631634\n300 0.0008039443637244403\n301 0.0007695338572375476\n302 0.0007362518226727843\n303 0.0007062728982418776\n304 0.0006773138884454966\n305 0.0006499512819573283\n306 0.0006234948523342609\n307 0.0005982205038890243\n308 0.0005752855213358998\n309 0.0005529063055291772\n310 0.0005300350021570921\n311 0.0005108935292810202\n312 0.0004914416931569576\n313 0.00047225007438100874\n3140.00045572081580758095\n315 0.0004389744717627764\n316 0.0004222285933792591\n317 0.000406899256631732\n318 0.00039281341014429927\n319 0.00037880742456763983\n320 0.0003654260071925819\n321 0.0003513795672915876\n322 0.00033950997749343514\n323 0.0003277435607742518\n324 0.00031675747595727444\n325 0.00030602200422436\n326 0.0002960807178169489\n327 0.00028741382993757725\n328 0.00027818340458907187\n329 0.0002690895344130695\n330 0.00026034953771159053\n331 0.0002524169394746423\n332 0.0002445413847453892\n333 0.00023706519277766347\n334 0.0002301162458024919\n335 0.00022240311955101788\n336 0.00021560511959251016\n337 0.00020928328740410507\n338 0.00020321049669291824\n339 0.00019715979578904808\n340 0.0001913408050313592\n341 0.00018579731113277376\n342 0.000180724062374793\n343 0.00017587744514457881\n344 0.00017091495101340115\n345 0.00016596310888417065\n346 0.00016102122026495636\n347 0.0001565898273838684\n348 0.00015272643940988928\n349 0.00014897588698659092\n350 0.00014455881319008768\n351 0.00014120734704192728\n352 0.00013769730867352337\n353 0.00013357264106161892\n354 0.00013053914881311357\n355 0.00012727148714475334\n356 0.00012355553917586803\n357 0.00012078398140147328\n358 0.00011774080485338345\n359 0.00011493984493426979\n360 0.00011201034794794396\n361 0.00010924593516392633\n362 0.0001065374817699194\n363 0.00010413508425699547\n364 0.0001018631155602634\n365 9.931954264175147e-05\n366 9.761117689777166e-05\n367 9.528947703074664e-05\n368 9.265482367482036e-05\n369 9.053402754943818e-05\n370 8.877013897290453e-05\n371 8.662263280712068e-05\n372 8.474809874314815e-05\n373 8.290501864394173e-05\n374 8.120127313304693e-05\n375 7.938798080431297e-05\n376 7.785463822074234e-05\n377 7.625405123690143e-05\n378 7.486288086511195e-05\n379 7.329093205044046e-05\n380 7.167555304476991e-05\n381 7.025210652500391e-05\n382 6.881653825985268e-05\n383 6.766541628167033e-05\n384 6.622137880185619e-05\n385 6.515398854389787e-05\n386 6.355606456054375e-05\n387 6.23706728219986e-05\n388 6.116929580457509e-05\n389 6.0166163166286424e-05\n390 5.915428846492432e-05\n391 5.808714922750369e-05\n392 5.684421194018796e-05\n393 5.57043676963076e-05\n394 5.48785574210342e-05\n395 5.39464526809752e-05\n396 5.2972354751545936e-05\n397 5.194980258238502e-05\n398 5.129476267029531e-05\n399 5.044039789936505e-05\n400 4.9474674597149715e-05\n401 4.8795136535773054e-05\n402 4.798932059202343e-05\n403 4.7080160584300756e-05\n404 4.648026151699014e-05\n405 4.589655145537108e-05\n406 4.489950879360549e-05\n407 4.409407119965181e-05\n408 4.329371586209163e-05\n409 4.241943679517135e-05\n410 4.196180452709086e-05\n411 4.1319137380924076e-05\n412 4.066883775522001e-05\n413 4.011277997051366e-05\n414 3.939835005439818e-05\n415 3.883121826220304e-05\n416 3.813135845120996e-05\n417 3.758858656510711e-05\n418 3.7105215596966445e-05\n419 3.6576086131390184e-05\n420 3.60465855919756e-05\n421 3.555036164470948e-05\n422 3.499468948575668e-05\n423 3.445546826696955e-05\n424 3.404730523470789e-05\n425 3.3573684049770236e-05\n426 3.308761733933352e-05\n427 3.267363354098052e-05\n428 3.200035644113086e-05\n429 3.1689291063230485e-05\n430 3.12690572172869e-05\n431 3.094713974860497e-05\n432 3.0647752282675356e-05\n433 3.0253386285039596e-05\n434 2.9859671485610306e-05\n435 2.9487619030987844e-05\n436 2.9156377422623336e-05\n437 2.8862339604529552e-05\n438 2.8517884857137688e-05\n439 2.8238760933163576e-05\n440 2.8009206289425492e-05\n441 2.7746973501052707e-05\n442 2.728282561292872e-05\n443 2.699440847209189e-05\n444 2.674087772902567e-05\n445 2.6180956410826184e-05\n446 2.5883760827127844e-05\n447 2.554181992309168e-05\n448 2.527032484067604e-05\n449 2.4923381715780124e-05\n450 2.4481301807099953e-05\n451 2.4317041606991552e-05\n452 2.404822953394614e-05\n453 2.375167969148606e-05\n454 2.3546886950498447e-05\n455 2.332849908270873e-05\n456 2.3084949134499766e-05\n457 2.2793159587308764e-05\n458 2.2429125237977132e-05\n459 2.218325244029984e-05\n460 2.1992997062625363e-05\n461 2.190701343351975e-05\n462 2.1603167624562047e-05\n463 2.1393620045273565e-05\n464 2.102657890645787e-05\n465 2.0817751646973193e-05\n466 2.0583383957273327e-05\n467 2.0453107936191373e-05\n468 2.016140024352353e-05\n469 1.997636354644783e-05\n470 1.973970211111009e-05\n471 1.95098873518873e-05\n472 1.928703932208009e-05\n473 1.9074133888352662e-05\n474 1.8947894204757176e-05\n475 1.879720002762042e-05\n476 1.8669205019250512e-05\n477 1.8535776689532213e-05\n478 1.8308224753127433e-05\n479 1.817997508624103e-05\n480 1.7991040294873528e-05\n481 1.7823058442445472e-05\n482 1.7719539755489677e-05\n483 1.749749935697764e-05\n484 1.7294996723649092e-05\n485 1.7145124729722738e-05\n486 1.696132676443085e-05\n487 1.6780408259364776e-05\n488 1.6752306692069396e-05\n489 1.6600599337834865e-05\n490 1.640201844566036e-05\n491 1.6287451217067428e-05\n492 1.6100997527246363e-05\n493 1.592815897311084e-05\n494 1.5705729310866445e-05\n495 1.5612431525369175e-05\n496 1.551580680825282e-05\n497 1.5311541574192233e-05\n498 1.5162077033892274e-05\n499 1.5139557945076376e-05\n"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.mm(w1)\n",
    "  h_relu = h.clamp(min=0)\n",
    "  y_pred = h_relu.mm(w2)\n",
    "\n",
    "  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "  # of shape (); we can get its value as a Python number with loss.item().\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "  grad_h = grad_h_relu.clone()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "  # Update weights using gradient descent\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 38833376.0\n1 34253576.0\n2 32893656.0\n3 28758416.0\n4 21138560.0\n5 12926023.0\n6 7098340.0\n7 3863841.25\n8 2275874.5\n9 1498081.25\n10 1089528.25\n11 848755.1875\n12 689814.75\n13 574813.0\n14 486429.0625\n15 415821.25\n16 358163.625\n17 310348.1875\n18 270318.15625\n19 236550.453125\n20 207873.125\n21 183347.640625\n22 162260.25\n23 144031.546875\n24 128239.0\n25 114513.109375\n26 102501.78125\n27 91959.0625\n28 82683.375\n29 74501.140625\n30 67253.8984375\n31 60837.265625\n32 55125.77734375\n33 50029.9921875\n34 45475.34765625\n35 41393.63671875\n36 37727.81640625\n37 34429.44140625\n38 31457.79296875\n39 28774.810546875\n40 26348.48828125\n41 24151.46484375\n42 22159.501953125\n43 20350.59765625\n44 18705.97265625\n45 17208.890625\n46 15844.314453125\n47 14599.224609375\n48 13462.18359375\n49 12422.3349609375\n50 11470.75390625\n51 10599.1513671875\n52 9800.73828125\n53 9068.4267578125\n54 8395.6533203125\n55 7777.2529296875\n56 7208.388671875\n57 6685.025390625\n58 6202.77490234375\n59 5758.19091796875\n60 5348.2685546875\n61 4969.662109375\n62 4619.88525390625\n63 4296.7109375\n64 3997.85791015625\n65 3721.8857421875\n66 3467.387451171875\n67 3231.654541015625\n68 3013.170166015625\n69 2810.548828125\n70 2622.473876953125\n71 2447.90966796875\n72 2285.74365234375\n73 2135.037841796875\n74 1994.9183349609375\n75 1864.6429443359375\n76 1743.378173828125\n77 1630.5128173828125\n78 1525.422119140625\n79 1427.56298828125\n80 1336.3358154296875\n81 1251.314697265625\n82 1172.0081787109375\n83 1098.024169921875\n84 1028.973876953125\n85 964.5218505859375\n86 904.3213500976562\n87 848.0855712890625\n88 795.5469360351562\n89 746.4397583007812\n90 700.515380859375\n91 657.5654296875\n92 617.39794921875\n93 579.8030395507812\n94 544.6196899414062\n95 511.6730041503906\n96 480.8247985839844\n97 451.9328918457031\n98 424.8607482910156\n99 399.4752197265625\n100 375.68682861328125\n101 353.3848571777344\n102 332.463623046875\n103 312.8354187011719\n104 294.42254638671875\n105 277.1527099609375\n106 260.93609619140625\n107 245.71324157714844\n108 231.41641235351562\n109 217.98634338378906\n110 205.37478637695312\n111 193.52163696289062\n112 182.38095092773438\n113 171.9081268310547\n114 162.0667266845703\n115 152.80868530273438\n116 144.10379028320312\n117 135.91342163085938\n118 128.20948791503906\n119 120.95787048339844\n120 114.13600158691406\n121 107.71389770507812\n122 101.6651611328125\n123 95.97079467773438\n124 90.60897064208984\n125 85.55831909179688\n126 80.79924011230469\n127 76.31449890136719\n128 72.09069061279297\n129 68.10847473144531\n130 64.35455322265625\n131 60.81440734863281\n132 57.477073669433594\n133 54.32904052734375\n134 51.35997009277344\n135 48.560035705566406\n136 45.91802215576172\n137 43.42500305175781\n138 41.07188415527344\n139 38.850528717041016\n140 36.754234313964844\n141 34.775577545166016\n142 32.90656661987305\n143 31.142032623291016\n144 29.474523544311523\n145 27.900035858154297\n146 26.412769317626953\n147 25.00707244873047\n148 23.679285049438477\n149 22.423473358154297\n150 21.237213134765625\n151 20.11624526977539\n152 19.056209564208984\n153 18.05330467224121\n154 17.105491638183594\n155 16.209243774414062\n156 15.361430168151855\n157 14.559252738952637\n158 13.800399780273438\n159 13.082138061523438\n160 12.40274429321289\n161 11.759834289550781\n162 11.151132583618164\n163 10.574974060058594\n164 10.029637336730957\n165 9.51340389251709\n166 9.024496078491211\n167 8.561522483825684\n168 8.123026847839355\n169 7.707826614379883\n170 7.314059257507324\n171 6.941409111022949\n172 6.588195323944092\n173 6.253915786743164\n174 5.936779975891113\n175 5.636074066162109\n176 5.3511786460876465\n177 5.081062316894531\n178 4.825355529785156\n179 4.582552433013916\n180 4.352427005767822\n181 4.134082317352295\n182 3.926985502243042\n183 3.7307019233703613\n184 3.5445168018341064\n185 3.367840528488159\n186 3.200071334838867\n187 3.041193962097168\n188 2.8902809619903564\n189 2.747129440307617\n190 2.6112310886383057\n191 2.4820785522460938\n192 2.359724760055542\n193 2.2435312271118164\n194 2.133131980895996\n195 2.028412342071533\n196 1.928924322128296\n197 1.8345407247543335\n198 1.7447329759597778\n199 1.6594502925872803\n200 1.5786159038543701\n201 1.501726746559143\n202 1.4286688566207886\n203 1.3593178987503052\n204 1.293318271636963\n205 1.230594277381897\n206 1.1711759567260742\n207 1.114519476890564\n208 1.0607303380966187\n209 1.0095698833465576\n210 0.9610692262649536\n211 0.9148541688919067\n212 0.8709597587585449\n213 0.829119861125946\n214 0.7894379496574402\n215 0.7516769766807556\n216 0.7157456874847412\n217 0.6816115379333496\n218 0.6490767002105713\n219 0.6182380318641663\n220 0.5888161063194275\n221 0.5608764290809631\n222 0.5342841148376465\n223 0.5089399218559265\n224 0.4848276376724243\n225 0.46190646290779114\n226 0.44010043144226074\n227 0.41939082741737366\n228 0.3996955156326294\n229 0.38079023361206055\n230 0.3628855347633362\n231 0.3458821475505829\n232 0.32963883876800537\n233 0.3141775131225586\n234 0.2994731366634369\n235 0.28547021746635437\n236 0.2721121311187744\n237 0.25940045714378357\n238 0.2473629117012024\n239 0.2358301877975464\n240 0.22488786280155182\n241 0.21443331241607666\n242 0.2044893503189087\n243 0.19496572017669678\n244 0.18592733144760132\n245 0.17735335230827332\n246 0.16914305090904236\n247 0.1613277941942215\n248 0.15386536717414856\n249 0.14679525792598724\n250 0.1399979293346405\n251 0.13359025120735168\n252 0.12743382155895233\n253 0.1215808317065239\n254 0.1160004511475563\n255 0.11067230999469757\n256 0.10561321675777435\n257 0.10076413303613663\n258 0.09615965932607651\n259 0.09180136024951935\n260 0.0876016765832901\n261 0.08361411094665527\n262 0.07979437708854675\n263 0.07618135958909988\n264 0.07270407676696777\n265 0.06941533088684082\n266 0.0662710964679718\n267 0.06326816976070404\n268 0.06040370464324951\n269 0.05768018215894699\n270 0.05506313592195511\n271 0.052563637495040894\n272 0.05021157115697861\n273 0.0479569211602211\n274 0.0457870252430439\n275 0.04372591897845268\n276 0.041758522391319275\n277 0.039891622960567474\n278 0.0380985401570797\n279 0.03639540821313858\n280 0.03475353121757507\n281 0.03320745751261711\n282 0.031717199832201004\n283 0.030316801741719246\n284 0.028957445174455643\n285 0.02766905538737774\n286 0.026440924033522606\n287 0.025264035910367966\n288 0.0241390410810709\n289 0.023080341517925262\n290 0.022051064297556877\n291 0.02107524685561657\n292 0.02015703171491623\n293 0.019270433112978935\n294 0.01841982826590538\n295 0.01759319007396698\n296 0.016824543476104736\n297 0.016084816306829453\n298 0.015390831045806408\n299 0.014719032682478428\n300 0.014077281579375267\n301 0.013465714640915394\n302 0.012881236150860786\n303 0.012311581522226334\n304 0.01177372969686985\n305 0.011271768249571323\n306 0.010782672092318535\n307 0.010321151465177536\n308 0.00988596398383379\n309 0.00945577397942543\n310 0.009049182757735252\n311 0.008667757734656334\n312 0.008293641731142998\n313 0.007946619763970375\n314 0.007612747140228748\n315 0.007290922105312347\n316 0.006982371676713228\n317 0.006688936613500118\n318 0.006408253684639931\n319 0.006141401827335358\n320 0.005886189639568329\n321 0.005638385657221079\n322 0.005402027629315853\n323 0.005183140281587839\n324 0.0049675083719193935\n325 0.00476468401029706\n326 0.004571261815726757\n327 0.004387818276882172\n328 0.004207744728773832\n329 0.004039872903376818\n330 0.0038730339147150517\n331 0.0037209326401352882\n332 0.003570952219888568\n333 0.003424394875764847\n334 0.0032941647805273533\n335 0.0031640762463212013\n336 0.0030416655354201794\n337 0.0029221144504845142\n338 0.002806981559842825\n339 0.002697532530874014\n340 0.0025938102044165134\n341 0.002492408500984311\n342 0.0023977847304195166\n343 0.0023071723990142345\n344 0.0022200257517397404\n345 0.002138343406841159\n346 0.0020567658357322216\n347 0.0019800448790192604\n348 0.0019081728532910347\n349 0.0018366093281656504\n350 0.0017702074255794287\n351 0.0017061998369172215\n352 0.0016438945895060897\n353 0.0015877967234700918\n354 0.0015304929111152887\n355 0.0014759872574359179\n356 0.0014217752031981945\n357 0.0013734411913901567\n358 0.001325238961726427\n359 0.0012776042567566037\n360 0.0012331808684393764\n361 0.0011902545811608434\n362 0.0011498486855998635\n363 0.0011105556041002274\n364 0.0010732722003012896\n365 0.0010387093061581254\n366 0.0010029975092038512\n367 0.0009705694392323494\n368 0.0009374653454869986\n369 0.0009074538247659802\n370 0.0008784674573689699\n371 0.0008500033291056752\n372 0.0008213067194446921\n373 0.0007958728820085526\n374 0.000770103360991925\n375 0.0007456980529241264\n376 0.0007226811721920967\n377 0.0007020433549769223\n378 0.0006783736171200871\n379 0.0006583506474271417\n380 0.0006376669625751674\n381 0.0006175899761728942\n382 0.0005993437953293324\n383 0.0005812061135657132\n384 0.0005643399199470878\n385 0.0005464072455652058\n386 0.0005306715611368418\n387 0.0005155985127203166\n388 0.000500183436088264\n389 0.00048590044025331736\n390 0.0004726263287011534\n391 0.0004595928476192057\n392 0.0004461788630578667\n393 0.00043332285713404417\n394 0.00042121001752093434\n395 0.0004097626660950482\n396 0.0003989179967902601\n397 0.000387752108508721\n398 0.000377263524569571\n399 0.00036739412462338805\n400 0.0003577861643861979\n401 0.0003482560277916491\n402 0.00033850717591121793\n403 0.0003302425902802497\n404 0.00032214008388109505\n405 0.00031415329431183636\n406 0.0003057655121665448\n407 0.00029916458879597485\n408 0.0002906430163420737\n409 0.00028291618218645453\n410 0.0002765718672890216\n411 0.0002695726288948208\n412 0.00026339630130678415\n413 0.00025627794093452394\n414 0.0002504409058019519\n415 0.0002446412399876863\n416 0.00023892460740171373\n417 0.00023343904467765242\n418 0.00022808738867752254\n419 0.00022281381825450808\n420 0.00021724100224673748\n421 0.00021202552306931466\n422 0.00020748675160575658\n423 0.00020320466137491167\n424 0.00019823116599582136\n425 0.0001940841757459566\n426 0.00018995279970113188\n427 0.0001858502218965441\n428 0.00018175612785853446\n429 0.00017810834106057882\n430 0.00017420537187717855\n431 0.00017073587514460087\n432 0.00016686093294993043\n433 0.00016335537657141685\n434 0.0001598854287294671\n435 0.00015660474309697747\n436 0.0001533768227091059\n437 0.0001505125837866217\n438 0.00014794510207138956\n439 0.00014493060007225722\n440 0.00014201369776856154\n441 0.0001390549005009234\n442 0.00013646956358570606\n443 0.00013370145461522043\n444 0.00013133641914464533\n445 0.00012903772585559636\n446 0.00012691310257650912\n447 0.0001242671423824504\n448 0.00012203594815218821\n449 0.000119519725558348\n450 0.00011787334369728342\n451 0.00011549425835255533\n452 0.00011348117550369352\n453 0.00011125222954433411\n454 0.00010919424676103517\n455 0.00010712643415899947\n456 0.0001050183636834845\n457 0.00010310150537407026\n458 0.00010144751286134124\n459 9.973470150725916e-05\n460 9.830161434365436e-05\n461 9.63589918683283e-05\n462 9.450523066334426e-05\n463 9.332886838819832e-05\n464 9.198709449265152e-05\n465 9.00923041626811e-05\n466 8.89755247044377e-05\n467 8.736358722671866e-05\n468 8.581610018154606e-05\n469 8.446270658168942e-05\n470 8.283240458695218e-05\n471 8.127177716232836e-05\n472 7.984842523001134e-05\n473 7.885935337981209e-05\n474 7.780589658068493e-05\n475 7.677081157453358e-05\n476 7.541268860222772e-05\n477 7.435830048052594e-05\n478 7.30047540855594e-05\n479 7.186552102211863e-05\n480 7.09871164872311e-05\n481 6.993998249527067e-05\n482 6.882977322675288e-05\n483 6.771426706109196e-05\n484 6.717276119161397e-05\n485 6.596816820092499e-05\n486 6.535014836117625e-05\n487 6.412922084564343e-05\n488 6.331508484436199e-05\n489 6.275007763179019e-05\n490 6.169821426738054e-05\n491 6.0766331444028765e-05\n492 5.9903886722167954e-05\n493 5.940950723015703e-05\n494 5.860669625690207e-05\n495 5.7653702242532745e-05\n496 5.6936536566354334e-05\n497 5.6268178013851866e-05\n498 5.537584365811199e-05\n499 5.463777779368684e-05\n"
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "  # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "  # PyTorch to build a computational graph, allowing automatic computation of\n",
    "  # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "  # don't need to keep references to intermediate values.\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "  # is a Python number giving its value.\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "  # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "  # of the loss with respect to w1 and w2 respectively.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent. For this step we just want to mutate\n",
    "  # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "  # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "  # to prevent PyTorch from building a computational graph for the updates\n",
    "  with torch.no_grad():\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 36505708.0\n1 31370954.0\n2 28555918.0\n3 24125838.0\n4 17863156.0\n5 11631517.0\n6 6983042.5\n7 4144855.0\n8 2575129.75\n9 1726715.5\n10 1250911.125\n11 963414.0\n12 774563.25\n13 640416.125\n14 539107.625\n15 459325.34375\n16 394710.25\n17 341417.71875\n18 296780.84375\n19 259102.875\n20 226987.4375\n21 199551.796875\n22 175993.3125\n23 155701.578125\n24 138124.6875\n25 122852.1015625\n26 109551.5078125\n27 97943.375\n28 87756.453125\n29 78801.4765625\n30 70903.5859375\n31 63924.953125\n32 57736.16015625\n33 52235.73046875\n34 47335.1015625\n35 42964.375\n36 39056.69921875\n37 35553.765625\n38 32405.841796875\n39 29573.81640625\n40 27021.083984375\n41 24714.958984375\n42 22630.25390625\n43 20743.33203125\n44 19037.8203125\n45 17489.85546875\n46 16082.564453125\n47 14801.1640625\n48 13633.60546875\n49 12568.6005859375\n50 11596.03515625\n51 10706.818359375\n52 9893.0009765625\n53 9147.2119140625\n54 8463.109375\n55 7835.59521484375\n56 7258.92822265625\n57 6728.8095703125\n58 6240.77685546875\n59 5791.54345703125\n60 5377.60546875\n61 4995.7841796875\n62 4643.72265625\n63 4318.78125\n64 4018.48681640625\n65 3740.876220703125\n66 3484.162109375\n67 3247.1201171875\n68 3027.5234375\n69 2823.923828125\n70 2635.022216796875\n71 2459.71728515625\n72 2296.990478515625\n73 2145.80029296875\n74 2005.2203369140625\n75 1874.5194091796875\n76 1752.8851318359375\n77 1639.6839599609375\n78 1534.248046875\n79 1436.043212890625\n80 1344.5325927734375\n81 1259.2413330078125\n821179.7579345703125\n83 1105.5330810546875\n84 1036.2474365234375\n85 971.5284423828125\n86 911.082275390625\n87 854.6058959960938\n88 801.7974853515625\n89 752.4286499023438\n90 706.2608032226562\n91 663.0664672851562\n92 622.6551513671875\n93 584.838134765625\n94 549.46533203125\n95 516.3425903320312\n96 485.30145263671875\n97 456.2171325683594\n98 428.9633483886719\n99 403.4085693359375\n100 379.43634033203125\n101 356.9461975097656\n102 335.8459777832031\n103 316.0434875488281\n104 297.4580993652344\n105 280.0080261230469\n106 263.6203918457031\n107 248.22848510742188\n108 233.76806640625\n109 220.1852569580078\n110 207.42349243164062\n111 195.42068481445312\n112 184.14002990722656\n113 173.53208923339844\n114 163.55918884277344\n115 154.18069458007812\n116 145.3550567626953\n117 137.04930114746094\n118 129.23452758789062\n119 121.87706756591797\n120 114.95429992675781\n121 108.4355697631836\n122 102.29655456542969\n123 96.51757049560547\n124 91.07350158691406\n125 85.94581604003906\n126 81.11528015136719\n127 76.56178283691406\n128 72.27252197265625\n129 68.23121643066406\n130 64.42096710205078\n131 60.82883071899414\n132 57.44499206542969\n133 54.25130081176758\n134 51.24058532714844\n135 48.40053176879883\n136 45.722145080566406\n137 43.194053649902344\n138 40.81094741821289\n139 38.56201934814453\n140 36.439327239990234\n141 34.43760681152344\n142 32.54692840576172\n143 30.7628231048584\n144 29.07871437072754\n145 27.488555908203125\n146 25.98736000061035\n147 24.570112228393555\n148 23.23103141784668\n149 21.967859268188477\n150 20.774431228637695\n151 19.646488189697266\n152 18.58124542236328\n153 17.574779510498047\n154 16.623886108398438\n155 15.725804328918457\n156 14.876913070678711\n157 14.074641227722168\n158 13.316391944885254\n159 12.59930419921875\n160 11.92187213897705\n161 11.281959533691406\n162 10.676597595214844\n163 10.104448318481445\n164 9.563155174255371\n165 9.051549911499023\n166 8.568078994750977\n167 8.110703468322754\n168 7.678273677825928\n169 7.26862907409668\n170 6.881837844848633\n171 6.515453338623047\n172 6.169145584106445\n173 5.841502666473389\n174 5.531702995300293\n175 5.238714218139648\n176 4.96088171005249\n177 4.69824743270874\n178 4.4495415687561035\n179 4.214423179626465\n180 3.9919209480285645\n181 3.7812647819519043\n182 3.581825017929077\n183 3.393153667449951\n184 3.2146148681640625\n185 3.0454185009002686\n186 2.8854596614837646\n187 2.733884334564209\n188 2.59024715423584\n189 2.4545295238494873\n190 2.3261280059814453\n191 2.2041659355163574\n192 2.0887064933776855\n193 1.979431390762329\n194 1.8759475946426392\n195 1.777849555015564\n196 1.685065746307373\n197 1.597195029258728\n198 1.5139167308807373\n199 1.4351412057876587\n200 1.3602583408355713\n201 1.2895959615707397\n202 1.2224565744400024\n203 1.1590203046798706\n204 1.0987780094146729\n205 1.0417906045913696\n206 0.9878095388412476\n207 0.9366151094436646\n208 0.8880122303962708\n209 0.8420652747154236\n210 0.7984362244606018\n211 0.7571483850479126\n212 0.7180353403091431\n213 0.6808986663818359\n214 0.6458439230918884\n215 0.6124572157859802\n216 0.5808606743812561\n217 0.5509381890296936\n218 0.5225431323051453\n219 0.49568116664886475\n220 0.4700831174850464\n221 0.445976585149765\n222 0.42302364110946655\n223 0.4012969434261322\n224 0.38070422410964966\n225 0.36119410395622253\n226 0.3426392078399658\n227 0.32504287362098694\n228 0.30837130546569824\n229 0.29264384508132935\n230 0.27762845158576965\n231 0.2633886933326721\n232 0.24998220801353455\n233 0.23716509342193604\n234 0.2250470519065857\n235 0.21354231238365173\n236 0.20265205204486847\n237 0.1923237442970276\n238 0.18253391981124878\n239 0.1732047200202942\n240 0.16439272463321686\n241 0.1560114622116089\n242 0.14807388186454773\n243 0.14053639769554138\n244 0.13336870074272156\n245 0.12657009065151215\n246 0.12014013528823853\n247 0.11407457292079926\n248 0.1082632839679718\n249 0.10276822000741959\n250 0.09753814339637756\n251 0.09259033203125\n252 0.08788642287254333\n253 0.08343721926212311\n254 0.07922231405973434\n255 0.07520081102848053\n256 0.07139387726783752\n257 0.06780633330345154\n258 0.06439796090126038\n259 0.061137422919273376\n260 0.058013010770082474\n261 0.0551166795194149\n262 0.05234069377183914\n263 0.049715425819158554\n264 0.04719385504722595\n265 0.04483626037836075\n266 0.042572446167469025\n267 0.040420446544885635\n268 0.03839917480945587\n269 0.036452677100896835\n270 0.03463614359498024\n271 0.03289211913943291\n272 0.031218044459819794\n273 0.029667990282177925\n274 0.028185753151774406\n275 0.026763778179883957\n276 0.02542640082538128\n277 0.024166205897927284\n278 0.022958669811487198\n279 0.02181185409426689\n280 0.020722808316349983\n281 0.019684309139847755\n282 0.01869717240333557\n283 0.01777355931699276\n284 0.016894016414880753\n285 0.016062526032328606\n286 0.015256219543516636\n287 0.014511453919112682\n288 0.013789848424494267\n289 0.013099395669996738\n290 0.012448242865502834\n291 0.011847268790006638\n292 0.01126742921769619\n293 0.010721771977841854\n294 0.010198627598583698\n295 0.009702288545668125\n296 0.009229027666151524\n297 0.00877644307911396\n298 0.008355546742677689\n299 0.007950988598167896\n300 0.007568947039544582\n301 0.007203522603958845\n302 0.006860425230115652\n303 0.0065338765271008015\n304 0.006224055774509907\n305 0.005923497956246138\n306 0.005642598029226065\n307 0.005384928081184626\n308 0.0051291389390826225\n309 0.004894849844276905\n310 0.0046648504212498665\n311 0.004447761923074722\n312 0.004243496805429459\n313 0.004048265516757965\n314 0.003862260142341256\n315 0.0036862168926745653\n316 0.0035182139836251736\n317 0.0033556593116372824\n318 0.003205964108929038\n319 0.0030621876940131187\n320 0.002926801098510623\n321 0.002796275308355689\n322 0.0026724461931735277\n323 0.0025560150388628244\n324 0.0024441129062324762\n325 0.002337415935471654\n326 0.002238039392977953\n327 0.0021417010575532913\n328 0.0020510212052613497\n329 0.001965062227100134\n330 0.0018818981479853392\n331 0.0018036536639556289\n332 0.0017315178411081433\n333 0.00165883160661906\n334 0.001594522618688643\n335 0.0015279371291399002\n336 0.001466343062929809\n337 0.0014076530933380127\n338 0.0013516860781237483\n339 0.0012971238465979695\n340 0.0012473356910049915\n341 0.001200301805511117\n342 0.0011528460308909416\n343 0.0011100375559180975\n344 0.0010663920547813177\n345 0.0010273756925016642\n346 0.0009908316424116492\n347 0.0009522593463771045\n348 0.0009179409244097769\n349 0.0008863414986990392\n350 0.0008535942761227489\n351 0.0008239979506470263\n352 0.0007945991237647831\n353 0.0007659093244001269\n354 0.0007395847351290286\n355 0.000714076217263937\n356 0.0006893795798532665\n357 0.0006656058831140399\n358 0.0006427100161090493\n359 0.0006222419906407595\n360 0.0006022797315381467\n361 0.0005814849864691496\n362 0.0005620167357847095\n363 0.0005446600262075663\n364 0.0005270678084343672\n365 0.0005088213365525007\n366 0.0004932970041409135\n367 0.0004789504164364189\n368 0.00046256373752839863\n369 0.00044858051114715636\n370 0.0004352859396021813\n371 0.00042270220001228154\n372 0.00040940725011751056\n373 0.0003977371088694781\n374 0.00038589737960137427\n375 0.0003743036650121212\n376 0.0003646956756711006\n377 0.000353465904481709\n378 0.0003432013909332454\n379 0.00033281801734119654\n380 0.000323771353578195\n381 0.00031503531499765813\n382 0.0003062231990043074\n383 0.00029763038037344813\n384 0.00029054348124191165\n385 0.00028285974985919893\n386 0.0002742319193203002\n387 0.00026681236340664327\n388 0.0002599988947622478\n389 0.0002534074883442372\n390 0.0002468189341016114\n391 0.00024068503989838064\n392 0.00023472766042687\n393 0.0002293744619237259\n394 0.0002238048182334751\n395 0.00021779394592158496\n396 0.00021270097931846976\n397 0.00020756261073984206\n398 0.00020241984748281538\n399 0.00019732308282982558\n400 0.00019293083460070193\n401 0.0001885171514004469\n402 0.00018400474800728261\n403 0.0001799288293113932\n404 0.000175417895661667\n405 0.00017160261631943285\n406 0.00016765114560257643\n407 0.0001642018323764205\n408 0.00016053162107709795\n409 0.00015795203216839582\n410 0.00015377563249785453\n411 0.0001502747181802988\n412 0.00014712871052324772\n413 0.00014384425594471395\n414 0.00014106174057815224\n415 0.000137953000376001\n416 0.00013498005864676088\n417 0.00013272996875457466\n418 0.00012965878704562783\n419 0.00012652292207349092\n420 0.00012435769895091653\n421 0.00012190827692393214\n422 0.00011958845425397158\n423 0.00011645531776593998\n424 0.00011450840975157917\n425 0.00011252840340603143\n426 0.0001104890980059281\n427 0.00010822425974765792\n428 0.00010590610327199101\n429 0.00010424460924696177\n430 0.00010237852984573692\n431 0.00010062646470032632\n432 9.866438631433994e-05\n433 9.666433470556512e-05\n434 9.511034295428544e-05\n435 9.336595394415781e-05\n436 9.144481737166643e-05\n437 8.992343646241352e-05\n438 8.816652552923188e-05\n439 8.691105904290453e-05\n440 8.516280649928376e-05\n441 8.387195703107864e-05\n442 8.280888869194314e-05\n443 8.154408715199679e-05\n444 7.976720371516421e-05\n445 7.798495789756998e-05\n446 7.711756916251034e-05\n447 7.572599861305207e-05\n448 7.440802437486127e-05\n449 7.353878027061e-05\n450 7.255290256580338e-05\n451 7.120456575648859e-05\n452 7.009685214143246e-05\n453 6.906172347953543e-05\n454 6.782486889278516e-05\n455 6.710015441058204e-05\n456 6.615388701902702e-05\n457 6.489123188657686e-05\n458 6.377188401529565e-05\n459 6.285800191108137e-05\n460 6.193794251885265e-05\n461 6.0943744756514207e-05\n462 6.01803621975705e-05\n463 5.9425627114251256e-05\n464 5.888097075512633e-05\n465 5.777852129540406e-05\n466 5.701021655113436e-05\n467 5.620267620543018e-05\n468 5.540900747291744e-05\n469 5.462774061015807e-05\n470 5.3867312090005726e-05\n471 5.2944611525163054e-05\n472 5.24821225553751e-05\n473 5.18341148563195e-05\n474 5.106163735035807e-05\n475 5.0555441703181714e-05\n476 4.9817521357908845e-05\n477 4.9393354856874794e-05\n478 4.879121479461901e-05\n479 4.803355841431767e-05\n480 4.730142973130569e-05\n481 4.675249510910362e-05\n482 4.6206234401324764e-05\n483 4.569671000353992e-05\n484 4.500347131397575e-05\n485 4.462272045202553e-05\n486 4.420622644829564e-05\n487 4.3519874452613294e-05\n488 4.310585791245103e-05\n489 4.257311229594052e-05\n490 4.216058732708916e-05\n491 4.157644070801325e-05\n492 4.110654117539525e-05\n493 4.065605389769189e-05\n494 4.0133636503014714e-05\n495 3.987341187894344e-05\n496 3.954565545427613e-05\n497 3.900566298398189e-05\n498 3.8582154957111925e-05\n499 3.804871812462807e-05\n"
    }
   ],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  We can implement our own custom autograd Functions by subclassing\n",
    "  torch.autograd.Function and implementing the forward and backward passes\n",
    "  which operate on Tensors.\n",
    "  \"\"\"\n",
    "  @staticmethod\n",
    "  def forward(ctx, x):\n",
    "    \"\"\"\n",
    "    In the forward pass we receive a context object and a Tensor containing the\n",
    "    input; we must return a Tensor containing the output, and we can use the\n",
    "    context object to cache objects for use in the backward pass.\n",
    "    \"\"\"\n",
    "    ctx.save_for_backward(x)\n",
    "    return x.clamp(min=0)\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive the context object and a Tensor containing\n",
    "    the gradient of the loss with respect to the output produced during the\n",
    "    forward pass. We can retrieve cached data from the context object, and must\n",
    "    compute and return the gradient of the loss with respect to the input to the\n",
    "    forward function.\n",
    "    \"\"\"\n",
    "    x, = ctx.saved_tensors\n",
    "    grad_x = grad_output.clone()\n",
    "    grad_x[x < 0] = 0\n",
    "    return grad_x\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and output\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Tensors; we call our\n",
    "  # custom ReLU implementation using the MyReLU.apply function\n",
    "  y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    " \n",
    "  # Compute and print loss\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass.\n",
    "  loss.backward()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 733.4744873046875\n1 715.1434326171875\n2 697.4378662109375\n3 680.3403930664062\n4 663.6915893554688\n5 647.5369262695312\n6 631.9785766601562\n7 616.8239135742188\n8 602.136962890625\n9 587.8182373046875\n10 573.8710327148438\n11 560.3023071289062\n12 547.04736328125\n13 534.1736450195312\n14 521.658447265625\n15 509.6225280761719\n16 497.893798828125\n17 486.5040588378906\n18 475.412841796875\n19 464.67926025390625\n20 454.2256164550781\n21 444.0511474609375\n22 434.153564453125\n23 424.4690246582031\n24 415.0345458984375\n25 405.8457336425781\n26 396.9121398925781\n27 388.23455810546875\n28 379.81622314453125\n29 371.57794189453125\n30 363.5458068847656\n31 355.7034912109375\n32 348.005859375\n33 340.435791015625\n34 333.0218811035156\n35 325.7752380371094\n36 318.6944580078125\n37 311.741943359375\n38 304.9066467285156\n39 298.2055969238281\n40 291.67181396484375\n41 285.2890319824219\n42 279.02197265625\n43 272.87542724609375\n44 266.8500061035156\n45 260.928466796875\n46 255.14207458496094\n47 249.4636688232422\n48 243.88455200195312\n49 238.4127655029297\n50 233.0414581298828\n51 227.76065063476562\n52 222.5762176513672\n53 217.485595703125\n54 212.5052490234375\n55 207.6072540283203\n56 202.79632568359375\n57 198.07704162597656\n58 193.44418334960938\n59 188.91661071777344\n60 184.47305297851562\n61 180.09634399414062\n62 175.8044891357422\n63 171.59957885742188\n64 167.46482849121094\n65 163.3994903564453\n66 159.40443420410156\n67 155.481201171875\n68 151.6459503173828\n69 147.8912353515625\n70 144.20672607421875\n71 140.59414672851562\n72 137.0557861328125\n73 133.5899200439453\n74 130.18881225585938\n75 126.83907318115234\n76 123.562255859375\n77 120.34542083740234\n78 117.1855239868164\n79 114.08208465576172\n80 111.0405502319336\n81 108.05866241455078\n82 105.13497924804688\n83 102.266845703125\n84 99.45533752441406\n85 96.70399475097656\n86 94.00443267822266\n87 91.37071228027344\n88 88.79132843017578\n89 86.26332092285156\n90 83.792724609375\n91 81.37883758544922\n92 79.01777648925781\n93 76.71226501464844\n94 74.4576187133789\n95 72.2548828125\n96 70.10284423828125\n97 68.00206756591797\n98 65.95242309570312\n99 63.951805114746094\n100 62.0007209777832\n101 60.09265899658203\n102 58.23299026489258\n103 56.4253044128418\n104 54.66032409667969\n105 52.94179153442383\n106 51.26856231689453\n107 49.6382942199707\n108 48.05023956298828\n109 46.50364303588867\n110 45.00049591064453\n111 43.53774642944336\n112 42.113807678222656\n113 40.727962493896484\n114 39.382293701171875\n115 38.07429885864258\n116 36.80595397949219\n117 35.57258987426758\n118 34.375972747802734\n119 33.213165283203125\n120 32.08716583251953\n121 30.99293327331543\n122 29.93208885192871\n123 28.902494430541992\n124 27.90325164794922\n125 26.9361629486084\n126 25.998294830322266\n127 25.08758544921875\n128 24.20546531677246\n129 23.351823806762695\n130 22.522499084472656\n131 21.720544815063477\n132 20.944290161132812\n133 20.192705154418945\n134 19.46504020690918\n135 18.760507583618164\n136 18.079736709594727\n137 17.420190811157227\n138 16.783124923706055\n139 16.167909622192383\n140 15.571332931518555\n141 14.993898391723633\n142 14.436464309692383\n143 13.89739990234375\n144 13.376248359680176\n145 12.871854782104492\n146 12.384993553161621\n147 11.915375709533691\n148 11.461652755737305\n149 11.022749900817871\n150 10.599056243896484\n151 10.19005298614502\n152 9.795344352722168\n153 9.415301322937012\n154 9.04797077178955\n155 8.693495750427246\n156 8.35168170928955\n157 8.0222749710083\n158 7.703990936279297\n159 7.397377014160156\n160 7.101641654968262\n161 6.816742897033691\n162 6.54219388961792\n163 6.277528762817383\n164 6.022709369659424\n165 5.777080535888672\n166 5.540799140930176\n167 5.313278675079346\n168 5.094498634338379\n169 4.8845648765563965\n170 4.682406902313232\n171 4.488008499145508\n172 4.301048278808594\n173 4.121020793914795\n174 3.948225498199463\n175 3.782158613204956\n176 3.6224660873413086\n177 3.4689600467681885\n178 3.3216307163238525\n179 3.1799709796905518\n180 3.0439670085906982\n181 2.9134583473205566\n182 2.788058280944824\n183 2.6678216457366943\n184 2.55238676071167\n185 2.4418489933013916\n186 2.3355867862701416\n187 2.233773708343506\n188 2.1362104415893555\n189 2.042675018310547\n190 1.9529855251312256\n191 1.8671010732650757\n192 1.7847442626953125\n193 1.7058954238891602\n194 1.6303808689117432\n195 1.558071494102478\n196 1.4888995885849\n197 1.4226455688476562\n198 1.359263300895691\n199 1.298575758934021\n200 1.240530014038086\n201 1.184941291809082\n202 1.1317713260650635\n203 1.0808601379394531\n204 1.0322198867797852\n205 0.9856134057044983\n206 0.9410536289215088\n207 0.8984551429748535\n208 0.8577613830566406\n209 0.818827748298645\n210 0.7816151976585388\n211 0.7460247278213501\n212 0.7120298147201538\n213 0.6795203685760498\n214 0.6484577059745789\n215 0.6187753677368164\n216 0.5904026627540588\n217 0.563295304775238\n218 0.5374090671539307\n219 0.51268470287323\n220 0.48904916644096375\n221 0.46648237109184265\n222 0.44492465257644653\n223 0.42434656620025635\n224 0.4047017991542816\n225 0.38599181175231934\n226 0.3681271970272064\n227 0.35105928778648376\n228 0.3347729444503784\n229 0.319223552942276\n230 0.3043818175792694\n231 0.29021209478378296\n232 0.2767029106616974\n233 0.2637963593006134\n234 0.25148406624794006\n235 0.2397339642047882\n236 0.22851215302944183\n237 0.21780787408351898\n238 0.20758533477783203\n239 0.19783535599708557\n240 0.18852810561656952\n241 0.1796465814113617\n242 0.17117656767368317\n243 0.1631004959344864\n244 0.15538696944713593\n245 0.14803382754325867\n246 0.14101794362068176\n247 0.13433106243610382\n248 0.12794066965579987\n249 0.12185671925544739\n250 0.11604013293981552\n251 0.1104992926120758\n252 0.10521693527698517\n253 0.10017602145671844\n254 0.09538663923740387\n255 0.09081990271806717\n256 0.08646546304225922\n257 0.08231545239686966\n258 0.07835838943719864\n259 0.07458575069904327\n260 0.07098957896232605\n261 0.06756209582090378\n262 0.06429675966501236\n263 0.061183881014585495\n264 0.05821869894862175\n265 0.05539335310459137\n266 0.05269906297326088\n267 0.050133805721998215\n268 0.04768882319331169\n269 0.04536014422774315\n270 0.04314012825489044\n271 0.04102611169219017\n272 0.03901220113039017\n273 0.03709407523274422\n274 0.03526782989501953\n275 0.03352809324860573\n276 0.031871404498815536\n277 0.030293898656964302\n278 0.028791924938559532\n279 0.027362294495105743\n280 0.026001187041401863\n281 0.024704905226826668\n282 0.023471686989068985\n283 0.022300858050584793\n284 0.021180834621191025\n285 0.020117979496717453\n286 0.01910644769668579\n287 0.018144238740205765\n288 0.01722892001271248\n289 0.016358116641640663\n290 0.015530060976743698\n291 0.014742107130587101\n292 0.013992867432534695\n293 0.013280846178531647\n294 0.012603521347045898\n295 0.01195957325398922\n296 0.011347170919179916\n297 0.010765108279883862\n298 0.010212031193077564\n299 0.009686063043773174\n300 0.009186528623104095\n301 0.008711649104952812\n302 0.008260592818260193\n303 0.007832047529518604\n304 0.0074250060133636\n305 0.007038250099867582\n306 0.006670960690826178\n307 0.006322105415165424\n308 0.005990924313664436\n309 0.005676485598087311\n310 0.005378037691116333\n311 0.005094713997095823\n312 0.00482572428882122\n313 0.004570480901747942\n314 0.004328286740928888\n315 0.004098475445061922\n316 0.003880480770021677\n317 0.003673601895570755\n318 0.0034774518571794033\n319 0.0032913738396018744\n320 0.0031149107962846756\n321 0.002947526052594185\n322 0.0027888636104762554\n323 0.0026385143864899874\n324 0.0024958732537925243\n325 0.00236079772002995\n326 0.002232733415439725\n327 0.002111416542902589\n328 0.001996417064219713\n329 0.001887488760985434\n330 0.0017842932138592005\n331 0.0016865419456735253\n332 0.0015939805889502168\n333 0.0015063532628118992\n334 0.0014233466936275363\n335 0.0013447798555716872\n336 0.0012703932588919997\n337 0.0011999929556623101\n338 0.0011333421571180224\n339 0.0010702855652198195\n340 0.0010106500703841448\n341 0.0009542055195197463\n342 0.0009008178021758795\n343 0.0008503133431077003\n344 0.0008025510469451547\n345 0.0007573970942758024\n346 0.0007146901334635913\n347 0.000674313516356051\n348 0.0006361497798934579\n349 0.0006000709254294634\n350 0.000565994530916214\n351 0.0005337844486348331\n352 0.000503353716339916\n353 0.0004746088234242052\n354 0.0004474600136745721\n355 0.00042181098251603544\n356 0.00039759569335728884\n357 0.0003747244772966951\n358 0.00035311616375111043\n359 0.0003327372542116791\n360 0.0003134784637950361\n361 0.00029530166648328304\n362 0.0002781673683784902\n363 0.00026198397972621024\n364 0.0002467190206516534\n365 0.0002323155349586159\n366 0.00021872116485610604\n367 0.00020591053180396557\n368 0.00019382525351829827\n369 0.00018242579244542867\n370 0.00017167498299386352\n371 0.00016154133481904864\n372 0.00015198889013845474\n373 0.00014298742462415248\n374 0.0001344989868812263\n375 0.0001265020837308839\n376 0.0001189658505609259\n377 0.00011187088239239529\n378 0.00010518676572246477\n379 9.889029752230272e-05\n380 9.295936615671962e-05\n381 8.7374632130377e-05\n382 8.211656677303836e-05\n383 7.716751133557409e-05\n384 7.25082354620099e-05\n385 6.812271021772176e-05\n386 6.39940844848752e-05\n387 6.010796641930938e-05\n388 5.645479905069806e-05\n389 5.3012023272458464e-05\n390 4.978098877472803e-05\n391 4.673681905842386e-05\n392 4.387549415696412e-05\n393 4.118317156098783e-05\n394 3.865189501084387e-05\n395 3.627562909969129e-05\n396 3.4040960599668324e-05\n397 3.1937655876390636e-05\n398 2.9962275220896117e-05\n399 2.8106163881602697e-05\n400 2.6362484277342446e-05\n401 2.47225852945121e-05\n402 2.318451697647106e-05\n403 2.1743206161772832e-05\n404 2.038839375018142e-05\n405 1.9116005205432884e-05\n406 1.7922266124514863e-05\n407 1.680073000898119e-05\n408 1.574842826812528e-05\n409 1.47594446389121e-05\n410 1.3831653632223606e-05\n411 1.2960987078258768e-05\n412 1.2143541425757576e-05\n413 1.1378145245544147e-05\n414 1.0659500730980653e-05\n415 9.984796633943915e-06\n416 9.350586879008915e-06\n417 8.757484465604648e-06\n418 8.200612683140207e-06\n419 7.678235306229908e-06\n420 7.189956249931129e-06\n421 6.729680535499938e-06\n422 6.299325832515024e-06\n423 5.896564289287198e-06\n424 5.518683792615775e-06\n425 5.1635101954161655e-06\n426 4.83215171698248e-06\n427 4.520538368524285e-06\n428 4.2284486880816985e-06\n429 3.955098691221792e-06\n430 3.6988803913118318e-06\n431 3.4587999380164547e-06\n432 3.234167252230691e-06\n433 3.024262014150736e-06\n434 2.827624257406569e-06\n435 2.64328946286696e-06\n436 2.4705807391001144e-06\n437 2.3089626211003633e-06\n438 2.1576952349278145e-06\n439 2.0164650322840316e-06\n440 1.8837669131244184e-06\n441 1.760094050951011e-06\n442 1.6437193153251428e-06\n443 1.5355943787653814e-06\n444 1.4340641882881755e-06\n445 1.3387990520641324e-06\n446 1.2506188795669004e-06\n447 1.1670877029246185e-06\n448 1.0897081210714532e-06\n449 1.0173774853683426e-06\n450 9.497238693256804e-07\n451 8.86053726389946e-07\n452 8.268872306871344e-07\n453 7.716134291513299e-07\n454 7.19710556040809e-07\n455 6.714785172334814e-07\n456 6.262618512664631e-07\n457 5.842562131874729e-07\n458 5.446553927868081e-07\n459 5.080262326373486e-07\n460 4.73903355668881e-07\n461 4.413460317209683e-07\n462 4.113672389394196e-07\n463 3.8363879184544203e-07\n464 3.575470657324331e-07\n465 3.332532401145727e-07\n466 3.104433403677831e-07\n467 2.8929181894454814e-07\n468 2.69646278638902e-07\n469 2.51026335718052e-07\n470 2.3396607673475955e-07\n471 2.1780643066904304e-07\n472 2.0297269998081902e-07\n473 1.8894941433700296e-07\n474 1.7581405131750216e-07\n475 1.6374474398617167e-07\n476 1.5240809148053813e-07\n477 1.4178846186041483e-07\n478 1.3201753290559282e-07\n479 1.228663819574649e-07\n480 1.1443561476198738e-07\n481 1.0637138814217906e-07\n482 9.899655140088726e-08\n483 9.209385609665333e-08\n484 8.572077092594554e-08\n485 7.970150761593686e-08\n486 7.417127534381507e-08\n487 6.889098358442425e-08\n488 6.407238117844827e-08\n489 5.956995252631714e-08\n490 5.5413561739214856e-08\n491 5.147449755327216e-08\n492 4.7901647093340216e-08\n493 4.4540414023686026e-08\n494 4.13941307897403e-08\n495 3.849732266303363e-08\n496 3.572563755938063e-08\n497 3.316860741620076e-08\n498 3.0860309863101065e-08\n499 2.868654647159019e-08\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y by passing x to the model.\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss.\n",
    "  loss = loss_fn(y_pred, y)\n",
    "  print(t, loss.item())\n",
    "  \n",
    "  # Before the backward pass, use the optimizer object to zero all of the\n",
    "  # gradients for the Tensors it will update (which are the learnable weights\n",
    "  # of the model)\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "  loss.backward()\n",
    "\n",
    "  # Calling the step function on an Optimizer makes an update to its parameters\n",
    "  optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonsai",
   "language": "python",
   "name": "bonsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}