{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.5 (default, Oct 25 2019, 15:51:11) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import PyTorch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.autograd import Function # import Function to create custom activations\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import datasets, transforms # import transformations to use for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_inf_to_zero(bad_tensor):\n",
    "    bad_tensor[bad_tensor == float('inf')] = 0\n",
    "    return bad_tensor\n",
    "\n",
    "\n",
    "# TODO: tests\n",
    "def calc_forces(state, prints = False):\n",
    "\n",
    "    aux1 = torch.tensor(([1.,0.],[0.,1.])) # used to transform state from [N_rows*2] to [N_rows*(2*N_rows)]\n",
    "    '''\n",
    "        e.g. : state   [[1,  0]\n",
    "                        [2,  1]\n",
    "                        [-1,-1]\n",
    "                        [0, -1]]\n",
    "        \n",
    "        new state_concated:         \n",
    "                       [[1,  0, 1,  0, 1,  0, 1,  0]\n",
    "                        [2,  1, 2,  1, 2,  1, 2,  1]\n",
    "                        [-1,-1, -1,-1, -1,-1, -1,-1]\n",
    "                        [0, -1, 0, -1, 0, -1, 0, -1]]\n",
    "                        \n",
    "\n",
    "    '''\n",
    "\n",
    "    for i in range(0,state.shape[0]-1):\n",
    "        aux1 = torch.cat((aux1,torch.tensor(([1.,0.],[0.,1.]))),dim=1)\n",
    "    \n",
    "    \n",
    "    aux1.requires_grad_(True)\n",
    "    aux1.retain_grad()\n",
    "    \n",
    "    state_concated = state.matmul(aux1).requires_grad_(True)\n",
    "    state_concated.retain_grad()\n",
    "    \n",
    "    state_concated_t = state.view(1,-1).requires_grad_(True)\n",
    "    for i in range(0,state.shape[0]-1):\n",
    "        state_concated_t = torch.cat([state_concated_t,state.view(1,-1)]).requires_grad_(True)\n",
    "        \n",
    "        '''    state_concated_t tensor(\n",
    "        [[ 1.,  0.,  2.,  1., -1., -1.,  0., -1.],\n",
    "         [ 1.,  0.,  2.,  1., -1., -1.,  0., -1.],\n",
    "         [ 1.,  0.,  2.,  1., -1., -1.,  0., -1.],\n",
    "         [ 1.,  0.,  2.,  1., -1., -1.,  0., -1.]]\n",
    "        '''\n",
    "    \n",
    "    state_concated_t.requires_grad_(True)\n",
    "\n",
    "    delta_pose = (state_concated_t - state_concated).requires_grad_(True)\n",
    "    delta_pose.retain_grad()\n",
    "    print (\"delta_pose \", delta_pose)\n",
    "    \n",
    "    auxullary = torch.zeros(state_concated.shape[0], state_concated.shape[1])\n",
    "    auxullary.retain_grad()\n",
    "    for i in range(state_concated.shape[0]):\n",
    "\n",
    "        auxullary[i,2*i] = 1.\n",
    "        auxullary[i,2*i+1] = 1.\n",
    "    print (\"auxullary \", auxullary)\n",
    "    ''' used to calc x+y of each agent pose\n",
    "        auxullary  tensor([\n",
    "            [1., 1., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 1., 1., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 1., 1., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 1., 1.]])\n",
    "    '''\n",
    "\n",
    "#     outputs = auxullary*delta_pose.requires_grad_(True)\n",
    "#     outputs.retain_grad()\n",
    "#     print (\"outputs \", outputs) \n",
    "#     s_d = torch.sum(outputs, dim=0).requires_grad_(True)\n",
    "#     s_d.retain_grad()\n",
    "#     print (\"s_d \", s_d) \n",
    "#     st2 = ((delta_pose - s_d)**2).requires_grad_(True)\n",
    "    dist_squared = ((delta_pose)**2).requires_grad_(True)\n",
    "    dist_squared.retain_grad()\n",
    "    print (\"dist_squared \", dist_squared) \n",
    "    \n",
    "    # used to calc delta_x**2 +delta_y**2 of each agent\n",
    "    aux = auxullary.t()\n",
    "    aux.retain_grad()\n",
    "    # sqrt(delta_x**2 +delta_y**2) -> distance\n",
    "    dist = torch.sqrt(dist_squared.matmul(aux)).requires_grad_(True) ## aka distance\n",
    "\n",
    "    print (\"dist \", dist)\n",
    "#     dist.retain_grad()\n",
    "\n",
    "    # force `amplitude` = 1/d\n",
    "    force_amplitude = tensor_inf_to_zero(1/dist).requires_grad_(True) ## aka 1/distance\n",
    "    force_amplitude.retain_grad()\n",
    "    \n",
    "    print (\"force_amplitude \", force_amplitude)\n",
    "    force_amplitude.backward(dist)\n",
    "    print (\"dist.grad \",dist.grad)\n",
    "\n",
    "    # f ~ 1/[d] * ([delta_pose]/[dist])\n",
    "    force = force_amplitude.matmul(auxullary) * delta_pose/dist.matmul(auxullary)\n",
    "\n",
    "    force[force != force] = 0\n",
    "    force.requires_grad_(True)\n",
    "    force.retain_grad()\n",
    "    print (\"force \", force)\n",
    "    force.backward(state_concated)\n",
    "    print (\"state_concated.grad \", state_concated.grad)\n",
    "    print (\"state_concated.requires_grad \", state_concated.requires_grad)\n",
    "    print (\"force.requires_grad \", force.requires_grad)\n",
    "    aux1 = aux1.t()\n",
    "    aux1.requires_grad_(True)\n",
    "    aux1.retain_grad()\n",
    "    force = force.matmul(aux1).requires_grad_(True)\n",
    "    force.retain_grad()\n",
    "    \n",
    "    \n",
    "    return force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_pose  tensor([[ 0.,  0.,  1.,  1., -2., -1., -1., -1.],\n",
      "        [-1., -1.,  0.,  0., -3., -2., -2., -2.],\n",
      "        [ 2.,  1.,  3.,  2.,  0.,  0.,  1.,  0.],\n",
      "        [ 1.,  1.,  2.,  2., -1.,  0.,  0.,  0.]], requires_grad=True)\n",
      "auxullary  tensor([[1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1.]])\n",
      "dist_squared  tensor([[0., 0., 1., 1., 4., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 9., 4., 4., 4.],\n",
      "        [4., 1., 9., 4., 0., 0., 1., 0.],\n",
      "        [1., 1., 4., 4., 1., 0., 0., 0.]], requires_grad=True)\n",
      "dist  tensor([[0.0000, 1.4142, 2.2361, 1.4142],\n",
      "        [1.4142, 0.0000, 3.6056, 2.8284],\n",
      "        [2.2361, 3.6056, 0.0000, 1.0000],\n",
      "        [1.4142, 2.8284, 1.0000, 0.0000]], requires_grad=True)\n",
      "force_amplitude  tensor([[0.0000, 0.7071, 0.4472, 0.7071],\n",
      "        [0.7071, 0.0000, 0.2774, 0.3536],\n",
      "        [0.4472, 0.2774, 0.0000, 1.0000],\n",
      "        [0.7071, 0.3536, 1.0000, 0.0000]], requires_grad=True)\n",
      "dist.grad  None\n",
      "force  tensor([[ 0.0000,  0.0000,  0.5000,  0.5000, -0.4000, -0.2000, -0.5000, -0.5000],\n",
      "        [-0.5000, -0.5000,  0.0000,  0.0000, -0.2308, -0.1538, -0.2500, -0.2500],\n",
      "        [ 0.4000,  0.2000,  0.2308,  0.1538,  0.0000,  0.0000,  1.0000,  0.0000],\n",
      "        [ 0.5000,  0.5000,  0.2500,  0.2500, -1.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "state_concated.grad  None\n",
      "state_concated.requires_grad  True\n",
      "force.requires_grad  True\n",
      "tensor([[-0.4000, -0.2000],\n",
      "        [-0.9808, -0.9038],\n",
      "        [ 1.6308,  0.3538],\n",
      "        [-0.2500,  0.7500]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "\n",
    "        output = calc_forces(input)\n",
    "        print (output)\n",
    "        output.retain_grad()\n",
    "        output.backward(input)\n",
    "        gr = input.grad \n",
    "        print (gr)\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]: \n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias    \n",
    "\n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "    \n",
    "l  = Linear(4,2, bias = None)\n",
    "\n",
    "data = torch.randn((4, 2),requires_grad=True)\n",
    "data = torch.tensor(([1.,  0],[2,  1],[-1,-1],[0, -1]))\n",
    "res = l(data)\n",
    "# print (data)\n",
    "# print ()\n",
    "# print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonsai",
   "language": "python",
   "name": "bonsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
